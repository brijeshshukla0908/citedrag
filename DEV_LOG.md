# DEV_LOG
# CitedRAG Development Log

> Track progress, decisions, and learnings throughout development

---

## 2025-12-29 - Session 1: Project Initialization ‚úÖ

### Completed
- ‚úÖ Decided on project name: **CitedRAG**
- ‚úÖ Created complete project structure
- ‚úÖ Set up requirements.txt with all dependencies
- ‚úÖ Configured config.py with validation
- ‚úÖ Created .env.example template
- ‚úÖ Set up .gitignore for security
- ‚úÖ Initialized development log

### Design Decisions
- **Name Choice**: CitedRAG over "Corporate Policy Engine"
  - Reason: Emphasizes technical innovation (citations) over narrow use case
  - Positions as general-purpose framework vs single-purpose tool
  - Better for portfolio at 4-year experience level

- **Tech Stack Finalized**:
  - LLM: Groq (Llama 3.1 8B) - 14.4K req/day free
  - Embeddings: Sentence Transformers (all-mpnet-base-v2) - local, free
  - Vector DB: ChromaDB - persistent, lightweight
  - Keyword: BM25 (rank-bm25) - hybrid retrieval
  - Evaluation: RAGAS - automated metrics
  - Hosting: Hugging Face Spaces - 16GB RAM free

### In Progress
- ‚è≥ Setting up local virtual environment
- ‚è≥ Installing dependencies
- ‚è≥ Testing Groq API connection

### Next Steps
1. Create Groq API test script
2. Implement document_processor.py (PDF extraction)
3. Test chunking strategy
4. Commit baseline code to Git

### Notes & Learnings
- Validated config.py includes checks for BM25+Vector weights = 1.0
- Added feature flags for easy toggling during development
- Storage directories auto-create on config import

### Time Spent: 2 hours

---

## 2025-12-29 - Session 2: Document Processing Module ‚úÖ

### Completed
- ‚úÖ Implemented document_processor.py with full functionality
- ‚úÖ PDF text extraction using PyMuPDF (fitz)
- ‚úÖ Text chunking with 500-token chunks and 100-token overlap
- ‚úÖ Metadata tracking (pages, tokens, file info)
- ‚úÖ Error handling for corrupted files and size limits
- ‚úÖ Unit tests (5 test cases, all passing)
- ‚úÖ Tested with real 12-page PDF successfully

### Key Achievements
- Successfully processed 12-page Nagarro Constitution document
- Created 15 chunks averaging 480 tokens each
- Token counting using tiktoken (cl100k_base encoding)
- Page number estimation for each chunk
- Standalone testing capability

### Technical Details
- Using PyMuPDF (fitz) for robust PDF extraction
- Token-based chunking (not character-based)
- Overlap ensures context preservation across chunks
- Metadata includes: filename, pages, tokens, chunk IDs, page numbers

### Next Steps
1. Implement embeddings.py (Sentence Transformers)
2. Generate embeddings for all chunks
3. Add batch processing for efficiency

### Time Spent: 30 minutes

---

## 2025-12-29 - Session 3: Embeddings Module ‚úÖ

### Completed
- ‚úÖ Implemented embeddings.py with Sentence Transformers
- ‚úÖ Model: sentence-transformers/all-mpnet-base-v2 (768-dim)
- ‚úÖ Batch embedding generation with progress tracking
- ‚úÖ Two-level caching (memory + disk)
- ‚úÖ Cosine similarity utility function
- ‚úÖ Full pipeline test: PDF ‚Üí Chunks ‚Üí Embeddings
- ‚úÖ Unit tests (6 test cases, all passing)

### Key Achievements
- Successfully embedded all 15 chunks from test PDF
- Cache system working (prevents re-embedding)
- Batch processing ~0.5 chunks/second
- Disk cache: ~6KB per embedding

### Technical Details
- Using SentenceTransformer library
- CPU-based inference (no GPU required)
- Caching via MD5 hash of text
- Pickle serialization for disk storage

### Performance
- 15 chunks embedded in ~2.5 seconds
- Cache hit rate: 100% on re-run
- Memory efficient (loads on demand)

### Next Steps
1. Implement vector_store.py (ChromaDB integration)
2. Store embeddings in vector database
3. Add similarity search functionality

### Time Spent: 30 minutes

---

## 2025-12-29 - Session 4: Vector Store (ChromaDB) ‚úÖ

### Completed
- ‚úÖ Implemented vector_store.py with ChromaDB
- ‚úÖ Persistent storage for embeddings and metadata
- ‚úÖ Similarity search (cosine distance)
- ‚úÖ Metadata filtering (by page, document, etc.)
- ‚úÖ Full pipeline integration: PDF ‚Üí Vector Store ‚Üí Search
- ‚úÖ Unit tests (6 test cases, all passing)

### Key Achievements
- Successfully stored 15 chunks with embeddings
- Semantic search working with real queries
- Metadata filtering operational
- Collection statistics tracking
- Document-level deletion support

### Technical Details
- Using ChromaDB with persistent client
- HNSW index for fast similarity search
- Cosine distance metric
- Automatic ID generation with timestamps
- Metadata includes: page, tokens, document name, model

### Performance
- 15 chunks stored in <1 second
- Search queries: ~50ms per query
- Storage: ~10KB per chunk (including metadata)
- Persistent across sessions

### Next Steps
1. Implement keyword_search.py (BM25)
2. Build hybrid retrieval (BM25 + Vector)
3. Create ensemble ranking system

### Time Spent: 30 minutes

---

## 2025-12-29 - Session 5: BM25 Keyword Search ‚úÖ

### Completed
- ‚úÖ Implemented keyword_search.py with BM25Okapi
- ‚úÖ Built BM25 index for 15 chunks from real PDF
- ‚úÖ Keyword-based search with relevance scoring
- ‚úÖ Index persistence (save/load from disk)
- ‚úÖ Integration with document processor
- ‚úÖ Unit tests (7 test cases, all passing)

### Key Achievements
- **BM25 indexing operational**: 15 chunks indexed in <0.1s
- **Keyword search working**: Queries like "employment policy" find exact matches
- **Complement to vector search**: Catches exact keyword matches vectors might miss
- **Index persistence**: Save/load index to avoid rebuilding

### Technical Details
- Using rank-bm25 library (BM25Okapi algorithm)
- Simple tokenization: lowercase + alphanumeric filtering
- Minimum token length: 3 characters
- Average 230 tokens per chunk in test document
- Index size: ~50KB for 15 chunks

### Performance
- Index building: <0.1 seconds for 15 chunks
- Search queries: <10ms per query
- Persistence: Save/load in <50ms

### Next Steps
1. Implement retrieval.py (Hybrid search)
2. Combine BM25 + Vector search with EnsembleRetriever
3. Weighted scoring and re-ranking

### Time Spent: 25 minutes

---

## 2025-12-29 - Session 6: Hybrid Retrieval ‚úÖ

### Completed
- ‚úÖ Implemented retrieval.py with hybrid search
- ‚úÖ Combined BM25 keyword + Vector semantic search
- ‚úÖ Ensemble ranking with weighted scoring
- ‚úÖ Score normalization (min-max scaling)
- ‚úÖ Re-ranking based on query term presence
- ‚úÖ Context generation for LLM prompts
- ‚úÖ Unit tests (6 test cases, all passing)

### Key Achievements
- **Hybrid search operational**: Best of both worlds (lexical + semantic)
- **Weighted scoring**: 50% BM25 + 50% Vector (configurable)
- **Ensemble ranking**: Merges results from both methods intelligently
- **Method tracking**: Identifies if chunk found by BM25, Vector, or both
- **Context formatting**: Ready for LLM consumption with citations

### Technical Details
- Retrieval method labeling: 'both', 'bm25_only', 'vector_only'
- Score normalization: Min-max scaling to 0-1 range
- Optional re-ranking: Boosts scores based on exact query term matches
- Context generation: Formatted with chunk IDs and page numbers
- Configurable weights via config.py

### Performance
- Hybrid search: ~60ms per query (BM25 + Vector combined)
- Better recall than either method alone
- Captures both exact matches and semantic relevance

### Example Results
Query: "employee conduct policy"
- Result 1: hybrid=0.72 (both methods) ‚Üê Best match
- Result 2: hybrid=0.68 (both methods) 
- Result 3: hybrid=0.57 (vector only) ‚Üê Semantic match

### Next Steps
1. Implement llm_handler.py (Groq API integration)
2. Build prompt templates
3. Generate answers with citations

### Time Spent: 30 minutes

---

## 2025-12-29 - Session 7: LLM Integration (Groq API) ‚úÖ

### Completed
- ‚úÖ Implemented llm_handler.py with Groq API
- ‚úÖ Answer generation with automatic citations
- ‚úÖ Citation extraction from LLM responses
- ‚úÖ Prompt engineering for accurate answers
- ‚úÖ Token usage tracking
- ‚úÖ Full end-to-end RAG pipeline operational
- ‚úÖ Unit tests (4 test cases, all passing)

### Key Achievements
- **Complete RAG system working**: PDF ‚Üí Retrieval ‚Üí LLM Answer
- **Automatic citations**: LLM cites sources using [Chunk X] notation
- **Fast inference**: Groq provides ~50 tokens/second
- **Token efficient**: Average 300-400 tokens per query
- **Full pipeline tested**: 3 questions answered successfully

### Technical Details
- Using Groq API with llama-3.1-8b-instant
- System prompt instructs LLM to cite sources
- Regex-based citation extraction
- Temperature: 0.3 for factual accuracy
- Max tokens: 500 for concise answers
- Streaming support (for future UI)

### Performance
- Answer generation: ~2-4 seconds per query
- Token usage: 200-250 prompt + 50-150 completion
- Citation accuracy: LLM reliably cites chunks
- Cost: ~$0.0001 per query (Groq pricing)

### Example Results
Question: "What is the Nagarro Constitution about?"
Answer: "The Nagarro Constitution is a global code of conduct handbook [Chunk 0]..."
Citations: 2 chunks cited, pages 1-2
Tokens: 312 total (245 prompt, 67 completion)

### Next Steps
1. Build CLI interface (cli.py)
2. Add FastAPI REST API (optional)
3. Create Gradio web UI (optional)
4. Deploy to production

### Time Spent: 35 minutes

---

## üéâ CITEDRAG CORE COMPLETE! üéâ

All 7 core modules implemented and tested:
1. ‚úÖ Document Processing (PDF ‚Üí Chunks)
2. ‚úÖ Embeddings (Sentence Transformers)
3. ‚úÖ Vector Store (ChromaDB)
4. ‚úÖ Keyword Search (BM25)
5. ‚úÖ Hybrid Retrieval (BM25 + Vector)
6. ‚úÖ LLM Integration (Groq API)
7. ‚úÖ Full RAG Pipeline (End-to-End)

**Total Development Time: ~3.5 hours**
**Test Coverage: 35+ unit tests, all passing**
**Status: Production-ready core system** ‚úÖ

---



### Completed
- 

### Issues Encountered
- 

### Solutions Applied
- 

### Next Steps
- 

### Time Spent:

---

## [Date] - Session 3: [Module Name]
[Template for future sessions]

---

## Development Metrics

| Metric | Target | Current |
|--------|--------|---------|
| **Core Modules Complete** | 8 | 0 |
| **Test Coverage** | >80% | 0% |
| **RAGAS Faithfulness** | >0.75 | - |
| **RAGAS Answer Relevance** | >0.75 | - |
| **Demo Questions Created** | 15 | 0 |
| **Total Dev Time** | ~80 hrs | 2 hrs |

---

## Issue Tracker

### Open Issues
- [ ] None yet

### Resolved Issues
- [x] None yet

---

## Ideas & Future Enhancements

- [ ] Multi-document comparison mode
- [ ] Conversational memory (follow-up questions)
- [ ] Query classification layer
- [ ] Fine-tuned embeddings for domain-specific docs
- [ ] A/B testing UI for BM25 vs Vector vs Hybrid
- [ ] Admin dashboard for usage analytics
- [ ] Webhook for integration with Slack/Teams
